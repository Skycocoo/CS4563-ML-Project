{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder to predict the next tweet of Donald Trump\n",
    "\n",
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data:  (30000,)\n",
      "0     I have not heard any of the pundits or comment...\n",
      "1     I would have done even better in the election,...\n",
      "2     Campaigning to win the Electoral College is mu...\n",
      "4     especially how to get people, even with an unl...\n",
      "5     Bill Clinton stated that I called him after th...\n",
      "6     \"@mike_pence: Congratulations to @RealDonaldTr...\n",
      "7     \"@Franklin_Graham: Congratulations to Presiden...\n",
      "8     We did it! Thank you to all of my great suppor...\n",
      "9     Today there were terror attacks in Turkey, Swi...\n",
      "10    If my many supporters acted and threatened peo...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_len = 30000\n",
    "num_words = 15000\n",
    "\n",
    "def load_file(file_name):\n",
    "    # convert all elements to string to avoid pandas dtype guessing\n",
    "    data = pd.read_csv(file_name,  converters={i: str for i in range(35000)})['Text']\n",
    "\n",
    "    # remove all retweets and replies\n",
    "    remove = (data.str.contains(\"RT\", case=True, na=False) | data.str.contains(\"RE\", case=True, na=False))\n",
    "    data = data[~remove] # ~: element-wise NOT operation\n",
    "\n",
    "    # remove all urls \n",
    "    # https://stackoverflow.com/questions/6883049/regex-to-extract-urls-from-href-attribute-in-html-with-python\n",
    "    data = data.str.replace(\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+(?:(\\/\\S+)*)\", \"\", regex=True) \n",
    "    return data\n",
    "\n",
    "data = load_file('./data/lessTweets.csv' )[: data_len]\n",
    "print(\"Total number of data: \", data.shape)\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# keep the most common 15000 words\n",
    "tokenizer = Tokenizer(num_words)\n",
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 35706\n",
      "Shape of data: 30000 sentences with at most 36 words\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "print(\"Unique tokens: {0}\".format(len(word2index)))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "# pad sequences to be the same length\n",
    "seq = pad_sequences(sequences)\n",
    "print(\"Shape of data: {0} sentences with at most {1} words\".format(seq.shape[0], seq.shape[1]))\n",
    "\n",
    "Tx = seq.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing word embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "embedding_index = {}\n",
    "embedding_dim = 25\n",
    "\n",
    "filePath = \"./data/glove.twitter.27B.\" + str(embedding_dim) + \"d.txt\"\n",
    "f = open(filePath)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embedding_index[word] = np.asarray(values[1:], dtype='float32') # Coefficients\n",
    "f.close()\n",
    "\n",
    "print('Found {0} word vectors.'.format(len(embedding_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FInish loading embedding weights from Glove\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word2index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix[i] = embedding_index.get('unk')\n",
    "            \n",
    "print(\"FInish loading embedding weights from Glove\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model layers for Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.layers.advanced_activations import ReLU\n",
    "from keras import backend as K\n",
    "\n",
    "n_a = 64 # number of hidden units\n",
    "n_l = 16 # number of latent units\n",
    "drop_rate = 0.2 # dropout rate\n",
    "batch_size = 100\n",
    "\n",
    "# initialize input layer\n",
    "x = Input(batch_shape=(None, Tx), name='input') \n",
    "# initialize word embedding layer with Glove weights\n",
    "x_embed = Embedding(num_words, embedding_dim, weights=[glove_embedding_matrix], input_length = Tx, trainable=False, name='embed')(x)\n",
    "\n",
    "# initialize bidirectional lstm encoder\n",
    "h = Bidirectional(LSTM(n_a, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat', name='lstm')(x_embed)\n",
    "# initialize dropout layer\n",
    "h = Dropout(drop_rate, name='dropout')(h)\n",
    "# initialize dense layer\n",
    "h = Dense(n_a, activation='linear', name='dense')(h)\n",
    "# initialize relu activication layer\n",
    "h = ReLU(name='relu')(h)\n",
    "# initialize another dense layer\n",
    "h = Dropout(drop_rate, name='dropout2')(h)\n",
    "\n",
    "# variational autoencoder: \n",
    "# z to describe the latent state\n",
    "z_mean = Dense(n_l, name='z_mean')(h)\n",
    "z_log_var = Dense(n_l, name='z_log_var')(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    # randomize sampling\n",
    "    epsilon = K.random_normal((K.shape(z_mean)[0], K.int_shape(z_mean)[1]))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# z contains two vectors to describe the latent state\n",
    "z = Lambda(sampling, output_shape=(n_l,))([z_mean, z_log_var])\n",
    "\n",
    "# repeat sample Tx times\n",
    "repeator = RepeatVector(Tx)\n",
    "# decode from latent space\n",
    "decoder_h = LSTM(n_a, return_sequences=True, recurrent_dropout=drop_rate)\n",
    "# TimeDistributed layer to keep one-to-one relations on input and output\n",
    "decoder_mean = TimeDistributed(Dense(num_words, activation='linear'))\n",
    "# decoded hidden layer\n",
    "h_decoded = decoder_h(repeator(z))\n",
    "# decoded output layer\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 36) (?, 36, 15000)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embed (Embedding)               (None, 36, 25)       375000      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm (Bidirectional)            (None, 128)          46080       embed[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu (ReLU)                     (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout2 (Dropout)              (None, 64)           0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 16)           1040        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 16)           1040        dropout2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_4 (RepeatVector)  (None, 36, 16)       0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 36, 64)       20736       repeat_vector_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 36, 15000)    975000      lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_16 (Cu [(None, 36), (None,  0           input[0][0]                      \n",
      "                                                                 time_distributed_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,427,152\n",
      "Trainable params: 1,052,152\n",
      "Non-trainable params: 375,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "# Custom VAE loss layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        self.target_weights = tf.constant(np.ones((batch_size, Tx)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
    "                                                     weights=self.target_weights,\n",
    "                                                     average_across_timesteps=False,\n",
    "                                                     average_across_batch=False), axis=-1)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape:\n",
    "        return K.ones_like(x)\n",
    "\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "opt = Adam(lr=0.01) #SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "vae.compile(optimizer='adam', loss=[zero_loss])\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoc  1 / 1\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 250s 1s/step - loss: 109.9283 - val_loss: 126.8649\n",
      "\n",
      "Epoch 00001: saving model to ./model/vae_seq2seq.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#vae = load_model('./model/vae.h5', custom_objects={\"CustomVariationalLayer\":CustomVariationalLayer(), \"zero_loss\":zero_loss})\n",
    "vae.load_weights('./model/vae_seq2seq.h5')\n",
    "epoch = 1\n",
    "seq_valid = seq[25000:] # validation data\n",
    "n_steps = (data_len - 5000) / batch_size # training data\n",
    "checkpointer = ModelCheckpoint('./model/vae_seq2seq.h5', verbose=1, save_best_only=False)\n",
    "\n",
    "def generator(seq, batch_size):\n",
    "    for i in range(batch_size, len(seq), batch_size):\n",
    "        yield[seq[i - batch_size: i], seq[i - batch_size: i]]\n",
    "    \n",
    "for i in range(epoch):\n",
    "    print(\"Epoc \", i+1,\"/\",epoch)\n",
    "    vae.fit_generator(generator(seq, batch_size), steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer], validation_data=(seq_valid, seq_valid))\n",
    "    \n",
    "vae.save('./model/vae2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 36) (?, 36, 15000)\n",
      "['18', '18', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see'] [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'loudobbs', 'hillary', 'just', 'handed', 'realdonaldtrump', 'a', 'huge', 'gift', 'promising', 'to', 'put', 'bubba', 'in', 'charge', 'of', 'the', 'economy', 'makeamericagreatagain']\n",
      "great great great great great a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:698: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
      "“patriotism “patriotism “patriotism “patriotism “patriotism “patriotism tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops\n",
      "“patriotism “patriotism “patriotism “patriotism “patriotism tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops\n",
      "-----------------\n",
      "great great great great great a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
      "great great great great a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
      "great great a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
      "great a a a a a tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops\n",
      "“patriotism “patriotism “patriotism “patriotism tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops\n",
      "“patriotism “patriotism “patriotism “patriotism “patriotism “patriotism tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops tops\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "from keras.models import load_model\n",
    "from random import randint\n",
    "load_model('./model/vae2.h5', custom_objects={\"CustomVariationalLayer\":CustomVariationalLayer(), \"zero_loss\":zero_loss})\n",
    "\n",
    "# build a model to project sentences on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# build a generator that can sample sentences from the learned distribution\n",
    "decoder_input = Input(shape=(n_l,))\n",
    "_h_decoded = decoder_h(repeator(decoder_input))\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "#index2word = {v: k for k, v in word2index.items()}\n",
    "sent_encoded = encoder.predict(seq_valid, batch_size = 16)\n",
    "x_test_reconstructed = generator.predict(sent_encoded)\n",
    "                                         \n",
    "sent_idx = randint(0,len(x_test_reconstructed)-1)\n",
    "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n",
    "\n",
    "\n",
    "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "original_sent = list(np.vectorize(index2word.get)(seq[sent_idx]))\n",
    "\n",
    "print(word_list, original_sent)\n",
    "\n",
    "# function to parse a sentence\n",
    "def sent_parse(sentence, mat_shape):\n",
    "    sequence = tokenizer.texts_to_sequences(sentence)\n",
    "    padded_sent = pad_sequences(sequence, maxlen=Tx)\n",
    "    return padded_sent#[padded_sent, sent_one_hot]\n",
    "\n",
    "# input: encoded sentence vector\n",
    "# output: encoded sentence vector in dataset with highest cosine similarity\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec\n",
    "\n",
    "# input: two points, integer n\n",
    "# output: n equidistant points on the line between the input points (inclusive)\n",
    "def shortest_homology(point_one, point_two, num):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = True)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample\n",
    "\n",
    "# input: original dimension sentence vector\n",
    "# output: sentence text\n",
    "def print_latent_sentence(sent_vect):\n",
    "    sent_vect = np.reshape(sent_vect,[1,n_l])\n",
    "    sent_reconstructed = generator.predict(sent_vect)\n",
    "    sent_reconstructed = np.reshape(sent_reconstructed,[Tx,num_words])\n",
    "    reconstructed_indexes = np.apply_along_axis(np.argmax, 1, sent_reconstructed)\n",
    "    np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx])\n",
    "    np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx]))\n",
    "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "    w_list = [w for w in word_list if w]\n",
    "    print(' '.join(w_list))\n",
    "        \n",
    "def new_sents_interp(sent1, sent2, n):\n",
    "    tok_sent1 = sent_parse(sent1, [15])\n",
    "    tok_sent2 = sent_parse(sent2, [15])\n",
    "    enc_sent1 = encoder.predict(tok_sent1, batch_size = 16)\n",
    "    enc_sent2 = encoder.predict(tok_sent2, batch_size = 16)\n",
    "    test_hom = shortest_homology(enc_sent1, enc_sent2, n)\n",
    "    for point in test_hom:\n",
    "        print_latent_sentence(point)\n",
    "\n",
    "sentence1=['Dossier allegations and now seriously doubts the Dossier claims. The whole Russian Collusion thing was a HOAX, but who is going to restore the good name of so many people whose reputations have been destroyed?']\n",
    "mysent = sent_parse(sentence1, [15])\n",
    "mysent_encoded = encoder.predict(mysent, batch_size = 100)\n",
    "print_latent_sentence(mysent_encoded)\n",
    "print_latent_sentence(find_similar_encoding(mysent_encoded))\n",
    "\n",
    "sentence2=[\"Tweets are great, and so is the united states\"]\n",
    "mysent2 = sent_parse(sentence2, [15])\n",
    "mysent_encoded2 = encoder.predict(mysent2, batch_size = 100)\n",
    "print_latent_sentence(mysent_encoded2)\n",
    "print_latent_sentence(find_similar_encoding(mysent_encoded2))\n",
    "print('-----------------')\n",
    "\n",
    "new_sents_interp(sentence1, sentence2, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
