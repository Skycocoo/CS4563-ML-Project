{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder to predict the next tweet of Donald Trump\n",
    "\n",
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data:  (30084,)\n",
      "0     I have not heard any of the pundits or comment...\n",
      "1     I would have done even better in the election,...\n",
      "2     Campaigning to win the Electoral College is mu...\n",
      "4     especially how to get people, even with an unl...\n",
      "5     Bill Clinton stated that I called him after th...\n",
      "6     \"@mike_pence: Congratulations to @RealDonaldTr...\n",
      "7     \"@Franklin_Graham: Congratulations to Presiden...\n",
      "8     We did it! Thank you to all of my great suppor...\n",
      "9     Today there were terror attacks in Turkey, Swi...\n",
      "10    If my many supporters acted and threatened peo...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_file(file_name):\n",
    "    # convert all elements to string to avoid pandas dtype guessing\n",
    "    data = pd.read_csv(file_name,  converters={i: str for i in range(35000)})['Text']\n",
    "\n",
    "    # remove all retweets and replies\n",
    "    remove = (data.str.contains(\"RT\", case=True, na=False) | data.str.contains(\"RE\", case=True, na=False))\n",
    "    data = data[~remove] # ~: element-wise NOT operation\n",
    "\n",
    "    # remove all urls \n",
    "    # https://stackoverflow.com/questions/6883049/regex-to-extract-urls-from-href-attribute-in-html-with-python\n",
    "    data = data.str.replace(\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+(?:(\\/\\S+)*)\", \"\", regex=True) \n",
    "    return data\n",
    "\n",
    "data= load_file('./data/lessTweets.csv' )\n",
    "print(\"Total number of data: \", data.shape)\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# keep the most common 20000 words\n",
    "tokenizer = Tokenizer(20000)\n",
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 35755\n",
      "Shape of data: 30084 sentences with 36 words at most\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "print(\"Unique tokens: {0}\".format(len(word2index)))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "# pad sequences to be the same length\n",
    "seq = pad_sequences(sequences)\n",
    "print(\"Shape of data: {0} sentences with {1} words at most\".format(seq.shape[0], seq.shape[1]))\n",
    "maxWords = seq.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing word embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "embeddingIdx = {}\n",
    "dims = 25\n",
    "filePath = \"./data/glove.twitter.27B.\" + str(dims) + \"d.txt\"\n",
    "f = open(filePath)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddingIdx[word] = np.asarray(values[1:], dtype='float32') # Coefficients\n",
    "f.close()\n",
    "\n",
    "print('Found {0} word vectors.'.format(len(embeddingIdx)))\n",
    "\n",
    "embeddingMtx = np.zeros((len(word2index) + 1, dims)) \n",
    "for word, i in word2index.items():\n",
    "    embeddingVec = embeddingIdx.get(word)\n",
    "    if embeddingVec is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading embedding matrix onto the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embeddingLayer = Embedding(len(word2index) + 1,dims, weights=[embeddingMtx],input_length=maxWords,trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
